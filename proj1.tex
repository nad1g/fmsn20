\documentclass[a4paper,10pt]{article}
\usepackage{amsmath}

%defines
\def\bY{{\bf Y}}
\def\bE{{\bf E}}
\def\bV{{\bf V}}
\def\bC{{\bf C}}
\def\b1{{\bf 1}}
\def\blambda{{\boldsymbol \lambda}}
\def\bgamma{{\boldsymbol \gamma}}
\def\bnu{{\boldsymbol \nu}}

%opening
\title{Home Assignment - 1}
\author{Santhosh Nadig}

\begin{document}

\maketitle

\section{Introduction}

\section{Theory}
Let $\bY_k$ represent the $n$ known observations of spatial data and $\bY_u$ represent the $m$ data points that needs to be estimated. For ordinary Kriging ($\mu = \b1 \beta$), the optimal predictions are given by
\begin{align*}
 \hat \bY_u &= \b1_u \hat \beta + \Sigma_{uk} \Sigma_{kk}^{-1}\left( \bY_k - \b1_k \hat \beta \right) \\
 \hat \beta &= \left( \b1_k^T \Sigma_{kk}^{-1} \b1_k \right)^{-1} \b1_k^T \Sigma_{kk}^{-1} \bY_k,
\end{align*}
where
\begin{align*}
 &\begin{bmatrix}
  \bY_k \\
  \bY_u
 \end{bmatrix} \in
 \mathbf{N} \left( 
 \begin{bmatrix}
    \b1_k \beta \\
   \b1_u \beta
  \end{bmatrix},
  \begin{bmatrix}
   \Sigma_{kk} & \Sigma_{ku} \\
   \Sigma_{uk} & \Sigma_{uu}
  \end{bmatrix}
  \right).
\end{align*}

Firstly, we show that the predictions are linear in the observations, i.e., $\hat \bY_u = \blambda^T \bY_k$ for some $\blambda$. We may re-write $\hat \beta$ as
\begin{align*}
 \hat \beta &= \bgamma^T \bY_k \\
 \bgamma &= \left( \left( \b1_k^T \Sigma_{kk}^{-1} \b1_k \right)^{-1} \b1_k^T \Sigma_{kk}^{-1} \right)^T.
\end{align*}
We note that the vector $\bgamma^T$ is of dimensions $1 \times n$, and therefore, $\hat \beta$ is a scalar. Substituting the above in the expression for $\hat \bY_u$, we obtain
\begin{align*}
 \hat \bY_u &= \b1_u \bgamma^T \bY_k + \Sigma_{uk} \Sigma_{kk}^{-1} \bY_k -  \Sigma_{uk} \Sigma_{kk}^{-1}  \b1_k \bgamma^T \bY_k \\
      &= \left( \b1_u \bgamma^T + \Sigma_{uk} \Sigma_{kk}^{-1} -  \Sigma_{uk} \Sigma_{kk}^{-1}  \b1_k \bgamma^T \right) \bY_k \\
      &= \blambda^T \bY_k,
\end{align*}
where $\blambda = \left( \b1_u \bgamma^T + \Sigma_{uk} \Sigma_{kk}^{-1} -  \Sigma_{uk} \Sigma_{kk}^{-1}  \b1_k \bgamma^T \right)^T$.

Secondly, we show that the predictions are unbiased, i.e., $\bE(\hat \bY_u) = \bE(\bY_u)$. The expected value of $\hat \beta$ is given by
\begin{align*}
 \bE \left( \hat \beta \right) &= \left( \b1_k^T \Sigma_{kk}^{-1} \b1_k \right)^{-1} \b1_k^T \Sigma_{kk}^{-1} ~ \bE (\bY_k) \\
  &= \left( \b1_k^T \Sigma_{kk}^{-1} \b1_k \right)^{-1} \b1_k^T \Sigma_{kk}^{-1} ~ \b1_k \beta \\
  &= \beta.
\end{align*}
Therefore,
\begin{align*}
 \bE(\hat \bY_u) &=  \b1_u \bE\left(\hat \beta \right) + \Sigma_{uk} \Sigma_{kk}^{-1}\left( \bE(\bY_k) - \b1_k \bE \left(\hat \beta \right)\right) \\
 &= \b1_u \beta + \Sigma_{uk} \Sigma_{kk}^{-1}\left( \b1_k \beta - \b1_k \beta \right) \\
 &= \b1_u \beta.
\end{align*}

Finally, consider a second unbiased predictor of $\bY_u$, given by $\tilde \bY_u = (\blambda + \bnu)^T \bY_k$. From the unbiasedness criterion, we have that $\bE(\tilde \bY_u) = \bE(\hat \bY_u)$. Therefore, $(\blambda + \bnu)^T \bE(\bY_k) = \blambda^T \bE(\bY_k)$, implying $\bnu^T \b1_k = 0$. Furthermore, consider the variance of the second predictor
\begin{align*}
 \bV(\tilde \bY_u - \bY_u) &= \bV(\bnu^T \bY_k + (\blambda^T \bY_k - \bY_u))\\
 &= \bV(\bnu^T \bY_k) + \bV(\hat \bY_u - \bY_u) + 2\bC(\bnu^T \bY_k, \hat \bY_u - \bY_u) \\
 &\ge \bV(\hat \bY_u - \bY_u) + 2\bC(\bnu^T \bY_k, \hat \bY_u - \bY_u)\\
 &= \bV(\hat \bY_u - \bY_u) + 2\bnu^T \bC(\bY_k, \hat \bY_u) - 2 \bnu^T \bC(\bY_k, \bY_u) \\
 &= \bV(\hat \bY_u - \bY_u) + 2\bnu^T \left(  \Sigma_{kk} \blambda - \Sigma_{uk}\right)
\end{align*}
Thus, if we can choose $\blambda$ such that $\bnu^T \left(  \Sigma_{kk} \blambda - \Sigma_{uk}\right) = 0$, then, we have that $\bV(\tilde \bY_u - \bY_u) \ge \bV(\hat \bY_u - \bY_u)$ for any unbiased predictor $\tilde \bY_u$.
\section{Data}

\section{Least-Squares}

\section{Universal Kriging}

\section{Conclusions}

\end{document}
